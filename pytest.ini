[pytest]
# Pytest configuration for performance optimization (Level 2.3)

minversion = 7.0

# Markers for test categorization
markers =
    slow: marks tests as slow (>1 sec)
    flaky: marks tests as flaky/unreliable
    integration: marks tests as integration tests (skip on PR)
    unit: marks tests as unit tests (run always)
    benchmark: marks tests as performance benchmarks

# Test discovery
python_files = test_*.py *_test.py tests.py
python_classes = Test*
python_functions = test_*

# Coverage options
addopts =
    -v
    --strict-markers
    --tb=short
    --disable-warnings
    --cov=src
    --cov=ml_training
    --cov-report=html:htmlcov
    --cov-report=term-missing:skip-covered
    --cov-fail-under=85
    --benchmark-min-rounds=1

# Timeout for tests (prevent hangs)
timeout = 300
timeout_method = thread

# Test paths
testpaths =
    tests
    src
    ml_training

# Ignore patterns
norecursedirs =
    .git
    .tox
    build
    dist
    *.egg
    .venv
    venv
    __pycache__
    .mypy_cache
    .pytest_cache
    htmlcov

# Logging
log_cli = false
log_cli_level = INFO
log_file = .pytest.log
log_file_level = DEBUG

# Parallel execution (via pytest-xdist)
# Run with: pytest -n auto
addopts = --benchmark-only --benchmark-autosave

# Performance thresholds
benchmark_min_rounds = 5
benchmark_compare = 0001
benchmark_compare_fail = mean:10%  # Fail if mean regression >10%
benchmark_warmup = true
